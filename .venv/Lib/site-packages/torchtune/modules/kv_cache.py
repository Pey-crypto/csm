# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

from typing import Tuple

import torch
from torch import nn


class KVCache(nn.Module):
    """
    Optimized KV-cache implementation for faster inference.
    
    Args:
        batch_size (int): batch size model will be run with
        max_seq_len (int): maximum sequence length model will be run with
        num_heads (int): number of heads
        head_dim (int): per-attention head embedding dimension
        dtype (torch.dtype): dtype for the caches
    """

    def __init__(
        self,
        batch_size: int,
        max_seq_len: int,
        num_heads: int,
        head_dim: int,
        dtype: torch.dtype,
    ) -> None:
        super().__init__()
        cache_shape = (batch_size, num_heads, max_seq_len, head_dim)
        self.register_buffer(
            "k_cache", torch.zeros(cache_shape, dtype=dtype), persistent=False
        )
        self.register_buffer(
            "v_cache", torch.zeros(cache_shape, dtype=dtype), persistent=False
        )
        # Simplified position tracking - just one scalar value
        self.register_buffer(
            "current_pos", torch.zeros(1, dtype=torch.long), persistent=False
        )
        self.batch_size = batch_size
        self.max_seq_len = max_seq_len

    def reset(self) -> None:
        """Reset the cache to zero."""
        # Only zero out the used portion for efficiency
        if self.size > 0:
            self.k_cache[:, :, :self.size] = 0
            self.v_cache[:, :, :self.size] = 0
        self.current_pos.zero_()

    @property
    def size(self) -> int:
        """Get the current cache size (number of tokens stored)."""
        return self.current_pos.item()

    def update(
        self, k_val: torch.Tensor, v_val: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Update KV cache with the new key-value pairs using optimized direct slicing.

        Args:
            k_val (torch.Tensor): Current key tensor with shape [B, H, S, D]
            v_val (torch.Tensor): Current value tensor with shape [B, H, S, D]

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: Updated key and value cache tensors

        Raises:
            ValueError: If batch size is too large for current cache
            AssertionError: If sequence length exceeds available cache space
        """
        bsz, _, seq_len, _ = k_val.shape
        if bsz > self.k_cache.shape[0]:
            raise ValueError(
                f"The current cache has been setup with a batch size of {self.k_cache.shape[0]}"
                f", but found new key tensors with batch size {k_val.shape[0]}!"
            )
        
        # Get current position as scalar for direct slicing
        start_pos = self.current_pos.item()
        
        # Ensure we don't exceed cache size
        assert (start_pos + seq_len) <= self.max_seq_len, "Exceeding maximum cache length"
        
        # Use direct slicing for faster updates (key optimization)
        self.k_cache[:bsz, :, start_pos:start_pos+seq_len] = k_val
        self.v_cache[:bsz, :, start_pos:start_pos+seq_len] = v_val
        
        # Update position counter
        self.current_pos += seq_len
        
        return self.k_cache, self.v_cache